% Chapter 18: Confronting the Partition Function

\chapter{Confronting the Partition Function}
\label{chap:partition-function}

This chapter addresses computational challenges in probabilistic models arising from intractable partition functions.

\section{The Partition Function Problem}
\label{sec:partition-problem}

Many models have form:
\begin{equation}
p(\vect{x}) = \frac{1}{Z} \tilde{p}(\vect{x})
\end{equation}

where partition function $Z = \sum_{\vect{x}} \tilde{p}(\vect{x})$ or $Z = \int \tilde{p}(\vect{x}) d\vect{x}$ is intractable.

\subsection{Why It's Hard}

Computing $Z$ requires:
\begin{itemize}
    \item Summing/integrating over all configurations
    \item Exponential in dimensionality
    \item Prohibitive for high-dimensional models
\end{itemize}

\subsection{Impact}

Cannot directly:
\begin{itemize}
    \item Evaluate likelihood $p(\vect{x})$
    \item Compute gradients for learning
    \item Compare models
\end{itemize}

\section{Contrastive Divergence}
\label{sec:contrastive-divergence}

\subsection{Motivation}

For Restricted Boltzmann Machines (RBMs):
\begin{equation}
p(\vect{v}, \vect{h}) = \frac{1}{Z} \exp(-E(\vect{v}, \vect{h}))
\end{equation}

Exact gradient requires expectations under model:
\begin{equation}
\frac{\partial \log p(\vect{v})}{\partial \theta} = -\mathbb{E}_{p(\vect{h}|\vect{v})}\left[\frac{\partial E}{\partial \theta}\right] + \mathbb{E}_{p(\vect{v}, \vect{h})}\left[\frac{\partial E}{\partial \theta}\right]
\end{equation}

\subsection{CD-k Algorithm}

Approximate second term with short MCMC chain (k steps):
\begin{enumerate}
    \item Start from data: $\vect{v}_0 = \vect{v}$
    \item Run k Gibbs steps
    \item Use $\vect{v}_k$ for negative phase
\end{enumerate}

Works surprisingly well despite being biased.

\section{Noise-Contrastive Estimation}
\label{sec:nce}

\subsection{Key Idea}

Turn density estimation into binary classification:
\begin{itemize}
    \item Distinguish data samples from noise samples
    \item Avoids computing partition function
\end{itemize}

\subsection{NCE Objective}

\begin{equation}
\mathcal{L} = \mathbb{E}_{p_{\text{data}}}[\log h(\vect{x})] + k \cdot \mathbb{E}_{p_{\text{noise}}}[\log(1-h(\vect{x}))]
\end{equation}

where:
\begin{equation}
h(\vect{x}) = \frac{p_{\text{model}}(\vect{x})}{p_{\text{model}}(\vect{x}) + k \cdot p_{\text{noise}}(\vect{x})}
\end{equation}

\subsection{Applications}

\begin{itemize}
    \item Word embeddings (word2vec)
    \item Language models
    \item Energy-based models
\end{itemize}

\section{Score Matching}
\label{sec:score-matching}

Match gradients of log-density (score function):
\begin{equation}
\psi(\vect{x}) = \nabla_{\vect{x}} \log p(\vect{x})
\end{equation}

Objective:
\begin{equation}
\mathcal{L} = \frac{1}{2} \mathbb{E}_{p_{\text{data}}}[\|\psi_{\theta}(\vect{x}) - \nabla_{\vect{x}} \log p_{\text{data}}(\vect{x})\|^2]
\end{equation}

Avoids partition function since it cancels in gradient.
