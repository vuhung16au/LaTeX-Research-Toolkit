% Chapter 3: Probability and Information Theory

\chapter{Probability and Information Theory}
\label{chap:probability}

This chapter introduces fundamental concepts from probability theory and information theory that are essential for understanding machine learning and deep learning. Topics include probability distributions, conditional probability, expectation, variance, entropy, and mutual information.

\section*{Learning Objectives}

After studying this chapter, you will be able to:

\begin{itemize}
    \item \textbf{Understand probability foundations}: Grasp the intuitive meaning of probability distributions, both discrete and continuous, and how they model uncertainty in real-world scenarios.
    
    \item \textbf{Apply conditional probability}: Use Bayes' theorem to update beliefs with new evidence and understand its central role in machine learning algorithms.
    
    \item \textbf{Calculate statistical measures}: Compute expectation, variance, and covariance to characterize the behavior of random variables and their relationships.
    
    \item \textbf{Work with common distributions}: Recognize when to use Bernoulli, Gaussian, and other probability distributions in machine learning contexts.
    
    \item \textbf{Quantify information content}: Use entropy, cross-entropy, and KL divergence to measure uncertainty and information in data and models.
    
    \item \textbf{Apply information theory}: Connect information-theoretic concepts to loss functions, model selection, and representation learning in deep neural networks.
\end{itemize}

\input{chapters/chap03-sec01}
\input{chapters/chap03-sec02}
\input{chapters/chap03-sec03}
\input{chapters/chap03-sec04}
\input{chapters/chap03-sec05}
\input{chapters/chap03-sec06}
