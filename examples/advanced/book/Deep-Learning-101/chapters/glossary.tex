% Glossary definitions for Deep Learning 101

% Basic Terms
\newglossaryentry{activation-function}{
    name={activation function},
    description={A function that determines the output of a neuron based on its input. Common examples include ReLU, sigmoid, and tanh.}
}

\newglossaryentry{backpropagation}{
    name={backpropagation},
    description={An algorithm for training neural networks that computes gradients by propagating errors backward through the network.}
}

\newglossaryentry{batch-normalization}{
    name={batch normalization},
    description={A technique that normalizes the inputs to each layer by adjusting and scaling the activations.}
}

\newglossaryentry{convolutional-neural-network}{
    name={convolutional neural network},
    description={A type of neural network designed for processing grid-like data such as images, using convolutional layers.}
}

\newglossaryentry{dropout}{
    name={dropout},
    description={A regularization technique that randomly sets a fraction of input units to 0 during training.}
}

\newglossaryentry{gradient-descent}{
    name={gradient descent},
    description={An optimization algorithm that iteratively adjusts parameters in the direction of steepest descent of the loss function.}
}

\newglossaryentry{loss-function}{
    name={loss function},
    description={A function that measures the difference between predicted and actual values, used to guide the learning process.}
}

\newglossaryentry{neural-network}{
    name={neural network},
    description={A computing system inspired by biological neural networks, consisting of interconnected nodes (neurons).}
}

\newglossaryentry{overfitting}{
    name={overfitting},
    description={A phenomenon where a model learns the training data too well, including noise, and performs poorly on new data.}
}

\newglossaryentry{regularization}{
    name={regularization},
    description={Techniques used to prevent overfitting by adding constraints or penalties to the model.}
}

\newglossaryentry{recurrent-neural-network}{
    name={recurrent neural network},
    description={A type of neural network designed for sequential data, where connections form directed cycles.}
}

\newglossaryentry{supervised-learning}{
    name={supervised learning},
    description={A machine learning approach where models learn from labeled training data.}
}

\newglossaryentry{unsupervised-learning}{
    name={unsupervised learning},
    description={A machine learning approach where models learn patterns from unlabeled data.}
}

\newglossaryentry{validation-set}{
    name={validation set},
    description={A subset of data used to evaluate model performance during training and tune hyperparameters.}
}

% Advanced Terms
\newglossaryentry{attention-mechanism}{
    name={attention mechanism},
    description={A technique that allows models to focus on relevant parts of the input when making predictions.}
}

\newglossaryentry{autoencoder}{
    name={autoencoder},
    description={A neural network architecture that learns to encode data into a lower-dimensional representation and then decode it back.}
}

\newglossaryentry{generative-adversarial-network}{
    name={generative adversarial network},
    description={A framework consisting of two neural networks competing against each other: a generator and a discriminator.}
}

\newglossaryentry{long-short-term-memory}{
    name={long short-term memory},
    description={A type of recurrent neural network architecture designed to overcome the vanishing gradient problem.}
}

\newglossaryentry{transformer}{
    name={transformer},
    description={A neural network architecture based on attention mechanisms, particularly effective for sequence modeling.}
}

\newglossaryentry{variational-autoencoder}{
    name={variational autoencoder},
    description={A generative model that learns to encode data into a probabilistic latent space and generate new samples.}
}

% Mathematical Terms
\newglossaryentry{gradient}{
    name={gradient},
    description={A vector of partial derivatives that points in the direction of steepest increase of a function.}
}

\newglossaryentry{hessian}{
    name={Hessian matrix},
    description={A square matrix of second-order partial derivatives of a scalar-valued function.}
}

\newglossaryentry{jacobian}{
    name={Jacobian matrix},
    description={A matrix of first-order partial derivatives of a vector-valued function.}
}

\newglossaryentry{optimization}{
    name={optimization},
    description={The process of finding the best parameters that minimize or maximize an objective function.}
}

% Application Terms
\newglossaryentry{computer-vision}{
    name={computer vision},
    description={A field of artificial intelligence that enables computers to interpret and understand visual information.}
}

\newglossaryentry{natural-language-processing}{
    name={natural language processing},
    description={A field of artificial intelligence that focuses on the interaction between computers and human language.}
}

\newglossaryentry{reinforcement-learning}{
    name={reinforcement learning},
    description={A machine learning paradigm where agents learn to make decisions through interaction with an environment.}
}
