% Chapter 15: Representation Learning

\chapter{Representation Learning}
\label{chap:representation-learning}

This chapter discusses the central challenge of deep learning: learning meaningful representations from data.

\section{What Makes a Good Representation?}
\label{sec:good-representations}

\subsection{Desirable Properties}

\textbf{Disentanglement:} Different factors of variation are separated
\begin{itemize}
    \item Changes in one dimension affect one factor
    \item Easier interpretation and manipulation
\end{itemize}

\textbf{Invariance:} Representation unchanged under irrelevant transformations
\begin{itemize}
    \item Translation, rotation invariance for objects
    \item Speaker invariance for speech content
\end{itemize}

\textbf{Smoothness:} Similar inputs have similar representations
\begin{itemize}
    \item Enables generalization
    \item Supports interpolation
\end{itemize}

\textbf{Sparsity:} Few features active for each input
\begin{itemize}
    \item Computational efficiency
    \item Interpretability
\end{itemize}

\subsection{Manifold Hypothesis}

Natural data lies on low-dimensional manifolds embedded in high-dimensional space.

Deep learning learns to:
\begin{itemize}
    \item Discover the manifold structure
    \item Map data to meaningful coordinates on manifold
\end{itemize}

\section{Transfer Learning and Domain Adaptation}
\label{sec:transfer-learning}

\subsection{Transfer Learning}

Leverage knowledge from source task to improve target task:

\textbf{Feature extraction:}
\begin{enumerate}
    \item Pre-train on large dataset (e.g., ImageNet)
    \item Freeze convolutional layers
    \item Train only final classification layers on target task
\end{enumerate}

\textbf{Fine-tuning:}
\begin{enumerate}
    \item Start with pre-trained model
    \item Continue training on target task with lower learning rate
    \item Optionally freeze early layers
\end{enumerate}

\subsection{Domain Adaptation}

Adapt model when training (source) and test (target) distributions differ.

\textbf{Approaches:}
\begin{itemize}
    \item \textbf{Domain-adversarial training:} Learn domain-invariant features
    \item \textbf{Self-training:} Use confident predictions on target domain
    \item \textbf{Multi-task learning:} Joint training on both domains
\end{itemize}

\subsection{Few-Shot Learning}

Learn from few examples per class:
\begin{itemize}
    \item \textbf{Meta-learning:} Learn to learn quickly (MAML)
    \item \textbf{Prototypical networks:} Learn metric space
    \item \textbf{Matching networks:} Attention-based comparison
\end{itemize}

\section{Self-Supervised Learning}
\label{sec:self-supervised}

Learn representations without manual labels by solving pretext tasks.

\subsection{Pretext Tasks}

\textbf{For images:}
\begin{itemize}
    \item \textbf{Rotation prediction:} Predict rotation angle
    \item \textbf{Jigsaw puzzle:} Arrange shuffled patches
    \item \textbf{Colorization:} Predict colors from grayscale
    \item \textbf{Inpainting:} Fill masked regions
\end{itemize}

\textbf{For text:}
\begin{itemize}
    \item \textbf{Masked language modeling:} Predict masked words (BERT)
    \item \textbf{Next sentence prediction:} Predict if sentences are consecutive
    \item \textbf{Autoregressive generation:} Predict next token (GPT)
\end{itemize}

\subsection{Benefits}

\begin{itemize}
    \item Leverage unlabeled data
    \item Learn general-purpose representations
    \item Often outperforms supervised pre-training
\end{itemize}

\section{Contrastive Learning}
\label{sec:contrastive-learning}

Learn representations by contrasting positive and negative pairs.

\subsection{Core Idea}

Maximize agreement between different views of same data (positive pairs), minimize agreement with other data (negative pairs).

\subsection{SimCLR Framework}

\begin{enumerate}
    \item Apply two random augmentations to each image
    \item Encode both views: $\vect{z}_i = f(\vect{x}_i)$, $\vect{z}_j = f(\vect{x}_j)$
    \item Minimize contrastive loss (NT-Xent):
\end{enumerate}

\begin{equation}
\ell_{i,j} = -\log \frac{\exp(\text{sim}(\vect{z}_i, \vect{z}_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{I}_{[k \neq i]} \exp(\text{sim}(\vect{z}_i, \vect{z}_k)/\tau)}
\end{equation}

where $\text{sim}(\cdot, \cdot)$ is cosine similarity and $\tau$ is temperature.

\subsection{MoCo (Momentum Contrast)}

Uses momentum encoder and queue of negative samples for efficiency.

\subsection{BYOL (Bootstrap Your Own Latent)}

Surprisingly, can work without negative samples using:
\begin{itemize}
    \item Online network (updated by gradients)
    \item Target network (momentum update)
    \item Prediction head on online network
\end{itemize}

\subsection{Applications}

State-of-the-art results in:
\begin{itemize}
    \item Image classification
    \item Object detection
    \item Segmentation
    \item Medical imaging with limited labels
\end{itemize}
