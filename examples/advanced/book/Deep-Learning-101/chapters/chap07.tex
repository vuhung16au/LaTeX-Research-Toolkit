% Chapter 7: Regularization for Deep Learning

\chapter{Regularization for Deep Learning}
\label{chap:regularization}

This chapter explores techniques to improve generalization and prevent overfitting in deep neural networks. Regularization helps models perform well on unseen data.

\section{Parameter Norm Penalties}
\label{sec:parameter-penalties}

\textbf{Parameter norm penalties} constrain model capacity by penalizing large weights.

\subsection{L2 Regularization (Weight Decay)}

Add squared L2 norm of weights to the loss:

\begin{equation}
\tilde{L}(\vect{\theta}) = L(\vect{\theta}) + \frac{\lambda}{2} \|\vect{w}\|^2
\end{equation}

Gradient update becomes:
\begin{equation}
\vect{w} \leftarrow (1 - \alpha\lambda)\vect{w} - \alpha \nabla_{\vect{w}} L
\end{equation}

The factor $(1 - \alpha\lambda)$ causes "weight decay."

\subsection{L1 Regularization}

Add L1 norm:
\begin{equation}
\tilde{L}(\vect{\theta}) = L(\vect{\theta}) + \lambda \|\vect{w}\|_1
\end{equation}

L1 regularization:
\begin{itemize}
    \item Promotes sparsity (many weights become exactly zero)
    \item Useful for feature selection
    \item Gradient: $\text{sign}(\vect{w})$
\end{itemize}

\subsection{Elastic Net}

Combines L1 and L2:
\begin{equation}
\tilde{L}(\vect{\theta}) = L(\vect{\theta}) + \lambda_1 \|\vect{w}\|_1 + \lambda_2 \|\vect{w}\|^2
\end{equation}

\section{Dataset Augmentation}
\label{sec:data-augmentation}

\textbf{Data augmentation} artificially increases training set size by applying transformations that preserve labels.

\subsection{Image Augmentation}

Common transformations:
\begin{itemize}
    \item \textbf{Geometric:} rotation, translation, scaling, flipping, cropping
    \item \textbf{Color:} brightness, contrast, saturation adjustments
    \item \textbf{Noise:} Gaussian noise, blur
    \item \textbf{Cutout/Erasing:} randomly mask regions
    \item \textbf{Mixup:} blend pairs of images and labels
\end{itemize}

Example: horizontal flip
\begin{equation}
\vect{x}_{\text{aug}} = \text{flip}(\vect{x}), \quad y_{\text{aug}} = y
\end{equation}

\subsection{Text Augmentation}

For NLP:
\begin{itemize}
    \item Synonym replacement
    \item Random insertion/deletion
    \item Back-translation
    \item Paraphrasing
\end{itemize}

\subsection{Audio Augmentation}

For speech/audio:
\begin{itemize}
    \item Time stretching
    \item Pitch shifting
    \item Adding background noise
    \item SpecAugment (masking frequency/time regions)
\end{itemize}

\section{Early Stopping}
\label{sec:early-stopping}

\textbf{Early stopping} monitors validation performance and stops training when it begins to degrade.

\subsection{Algorithm}

\begin{enumerate}
    \item Train model and evaluate on validation set periodically
    \item Track best validation performance
    \item If no improvement for $p$ epochs (patience), stop
    \item Return model with best validation performance
\end{enumerate}

\subsection{Benefits}

\begin{itemize}
    \item Simple and effective
    \item Automatically determines training duration
    \item Acts as regularization without changing the model
    \item Reduces computational cost
\end{itemize}

\subsection{Considerations}

\begin{itemize}
    \item Requires separate validation set
    \item Choice of patience hyperparameter
    \item Can interact with learning rate schedules
\end{itemize}

\section{Dropout}
\label{sec:dropout}

\textbf{Dropout} randomly deactivates neurons during training, preventing co-adaptation.

\subsection{Training with Dropout}

At each training step, for each layer:
\begin{enumerate}
    \item Sample binary mask $\vect{m}$ with $P(m_i = 1) = p$
    \item Apply mask: $\vect{h} = \vect{m} \odot \vect{h}$
\end{enumerate}

Mathematically:
\begin{equation}
\vect{h}_{\text{dropout}} = \vect{m} \odot f(\mat{W}\vect{x} + \vect{b})
\end{equation}

where $m_i \sim \text{Bernoulli}(p)$.

\subsection{Inference}

At test time, scale outputs by dropout probability:
\begin{equation}
\vect{h}_{\text{test}} = p \cdot f(\mat{W}\vect{x} + \vect{b})
\end{equation}

Or equivalently, scale weights during training by $\frac{1}{p}$ (inverted dropout).

\subsection{Interpretation}

Dropout can be viewed as:
\begin{itemize}
    \item Training an ensemble of $2^n$ subnetworks
    \item Adding noise to hidden activations
    \item Approximate Bayesian inference
\end{itemize}

\subsection{Variants}

\textbf{DropConnect:} drop weights instead of activations

\textbf{Spatial Dropout:} drop entire feature maps in CNNs

\textbf{Variational Dropout:} use same mask across time steps in RNNs

\section{Batch Normalization}
\label{sec:batch-normalization}

\textbf{Batch normalization} normalizes layer inputs across the batch dimension.

\subsection{Algorithm}

For mini-batch $\mathcal{B}$ with activations $\vect{x}$:

\begin{align}
\mu_{\mathcal{B}} &= \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} x_i \\
\sigma^2_{\mathcal{B}} &= \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} (x_i - \mu_{\mathcal{B}})^2 \\
\hat{x}_i &= \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma^2_{\mathcal{B}} + \epsilon}} \\
y_i &= \gamma \hat{x}_i + \beta
\end{align}

where $\gamma$ and $\beta$ are learnable parameters.

\subsection{Benefits}

\begin{itemize}
    \item Reduces internal covariate shift
    \item Allows higher learning rates
    \item Reduces sensitivity to initialization
    \item Acts as regularization
    \item Enables deeper networks
\end{itemize}

\subsection{Inference}

At test time, use running averages computed during training:
\begin{equation}
y = \gamma \frac{x - \mu_{\text{running}}}{\sqrt{\sigma^2_{\text{running}} + \epsilon}} + \beta
\end{equation}

\subsection{Variants}

\textbf{Layer Normalization:} normalize across features (useful for RNNs)

\textbf{Group Normalization:} normalize within groups of channels

\textbf{Instance Normalization:} normalize each sample independently (used in style transfer)

\section{Other Regularization Techniques}
\label{sec:other-regularization}

\subsection{Label Smoothing}

Replace hard targets with smoothed distributions:
\begin{equation}
y'_k = (1 - \epsilon) y_k + \frac{\epsilon}{K}
\end{equation}

Prevents overconfident predictions.

\subsection{Gradient Clipping}

Limit gradient magnitude to prevent exploding gradients:

\textbf{Clipping by value:}
\begin{equation}
g \leftarrow \max(\min(g, \theta), -\theta)
\end{equation}

\textbf{Clipping by norm:}
\begin{equation}
g \leftarrow \frac{g}{\max(1, \|g\| / \theta)}
\end{equation}

\subsection{Stochastic Depth}

Randomly skip layers during training (for very deep networks).

\subsection{Mixup}

Train on convex combinations of examples:
\begin{align}
\tilde{\vect{x}} &= \lambda \vect{x}_i + (1-\lambda) \vect{x}_j \\
\tilde{y} &= \lambda y_i + (1-\lambda) y_j
\end{align}

where $\lambda \sim \text{Beta}(\alpha, \alpha)$.

\subsection{Adversarial Training}

Add adversarially perturbed examples to training:
\begin{equation}
\vect{x}_{\text{adv}} = \vect{x} + \epsilon \cdot \text{sign}(\nabla_{\vect{x}} L(\vect{x}, y))
\end{equation}

Improves robustness and generalization.
