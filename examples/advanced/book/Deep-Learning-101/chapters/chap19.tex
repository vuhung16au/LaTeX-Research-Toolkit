% Chapter 19: Approximate Inference

\chapter{Approximate Inference}
\label{chap:approximate-inference}

This chapter explores methods for tractable inference in complex probabilistic models.

\section{Variational Inference}
\label{sec:variational-inference}

\subsection{Evidence Lower Bound (ELBO)}

For latent variable model with intractable posterior $p(\vect{z}|\vect{x})$, approximate with $q(\vect{z})$:

\begin{align}
\log p(\vect{x}) &= \mathbb{E}_{q(\vect{z})}[\log p(\vect{x})] \\
&= \mathbb{E}_{q(\vect{z})}\left[\log \frac{p(\vect{x}, \vect{z})}{p(\vect{z}|\vect{x})}\right] \\
&= \mathbb{E}_{q(\vect{z})}\left[\log \frac{p(\vect{x}, \vect{z})}{q(\vect{z})}\right] + D_{KL}(q(\vect{z}) \| p(\vect{z}|\vect{x})) \\
&\geq \mathbb{E}_{q(\vect{z})}\left[\log \frac{p(\vect{x}, \vect{z})}{q(\vect{z})}\right] = \mathcal{L}(q)
\end{align}

Maximizing $\mathcal{L}(q)$ minimizes $D_{KL}(q(\vect{z}) \| p(\vect{z}|\vect{x}))$.

\subsection{Variational Family}

Choose tractable family of distributions:

\textbf{Mean field:} Fully factorized
\begin{equation}
q(\vect{z}) = \prod_{i=1}^{n} q_i(z_i)
\end{equation}

\textbf{Structured:} Allow some dependencies
\begin{equation}
q(\vect{z}) = \prod_{c} q_c(\vect{z}_c)
\end{equation}

Trade-off between expressiveness and tractability.

\subsection{Coordinate Ascent VI}

Optimize each factor iteratively:
\begin{equation}
q_j^*(z_j) \propto \exp\left(\mathbb{E}_{q_{-j}}[\log p(\vect{z}, \vect{x})]\right)
\end{equation}

Guaranteed to converge to local optimum of ELBO.

\subsection{Stochastic Variational Inference}

Use stochastic gradients for scalability:
\begin{itemize}
    \item Mini-batch data
    \item Monte Carlo estimation of expectations
    \item Reparameterization trick for low variance
\end{itemize}

\section{Mean Field Approximation}
\label{sec:mean-field}

\subsection{Fully Factorized Approximation}

Assume all variables independent:
\begin{equation}
q(\vect{z}) = \prod_{i=1}^{n} q_i(z_i)
\end{equation}

\subsection{Update Equations}

For each variable:
\begin{equation}
\log q_j^*(z_j) = \mathbb{E}_{i \neq j}[\log p(\vect{z}, \vect{x})] + \text{const}
\end{equation}

Iterate until convergence.

\subsection{Properties}

\begin{itemize}
    \item Underestimates variance (overconfident)
    \item Computationally efficient
    \item Often good approximation in practice
\end{itemize}

\section{Loopy Belief Propagation}
\label{sec:loopy-bp}

\subsection{Message Passing}

On graphical models, pass messages between nodes:
\begin{equation}
m_{i \to j}(x_j) = \sum_{x_i} \psi(x_i, x_j) \psi(x_i) \prod_{k \in N(i) \setminus j} m_{k \to i}(x_i)
\end{equation}

\subsection{Beliefs}

Compute marginals from messages:
\begin{equation}
b_i(x_i) \propto \psi(x_i) \prod_{j \in N(i)} m_{j \to i}(x_i)
\end{equation}

\subsection{Exact on Trees}

For tree-structured graphs, converges to exact marginals.

\subsection{Loopy Graphs}

On graphs with cycles:
\begin{itemize}
    \item May not converge
    \item Often gives good approximations
    \item Used in error-correcting codes, computer vision
\end{itemize}

\section{Expectation Propagation}
\label{sec:ep}

Approximates each factor with simpler distribution:
\begin{equation}
p(\vect{x}) = \frac{1}{Z} \prod_i f_i(\vect{x}) \approx \frac{1}{Z} \prod_i \tilde{f}_i(\vect{x})
\end{equation}

Iteratively refine approximations to match moments.

Better than mean field for multi-modal posteriors.
