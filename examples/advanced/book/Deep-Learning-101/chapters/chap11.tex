% Chapter 11: Practical Methodology

\chapter{Practical Methodology}
\label{chap:practical-methodology}

This chapter provides practical guidelines for successfully applying deep learning to real-world problems.

\section{Performance Metrics}
\label{sec:performance-metrics}

\subsection{Classification Metrics}

\textbf{Accuracy:}
\begin{equation}
\text{Accuracy} = \frac{\text{Correct predictions}}{\text{Total predictions}}
\end{equation}

\textbf{Precision and Recall:}
\begin{align}
\text{Precision} &= \frac{\text{TP}}{\text{TP} + \text{FP}} \\
\text{Recall} &= \frac{\text{TP}}{\text{TP} + \text{FN}}
\end{align}

\textbf{F1 Score:} Harmonic mean of precision and recall
\begin{equation}
F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

\textbf{ROC Curve and AUC:} Trade-off between true positive rate and false positive rate

\textbf{Confusion Matrix:} Visualizes prediction performance across classes

\subsection{Regression Metrics}

\textbf{Mean Squared Error (MSE):}
\begin{equation}
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\end{equation}

\textbf{Mean Absolute Error (MAE):}
\begin{equation}
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\end{equation}

\textbf{R-squared ($R^2$):} Proportion of variance explained

\subsection{NLP Metrics}

\textbf{BLEU:} For machine translation (measures n-gram overlap)

\textbf{ROUGE:} For summarization

\textbf{Perplexity:} For language models
\begin{equation}
\text{PPL} = \exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log P(x_i)\right)
\end{equation}

\section{Baseline Models and Debugging}
\label{sec:baselines-debugging}

\subsection{Establishing Baselines}

Start with simple baselines:
\begin{enumerate}
    \item \textbf{Random baseline:} Random predictions
    \item \textbf{Simple heuristics:} Rule-based systems
    \item \textbf{Classical ML:} Logistic regression, random forests
    \item \textbf{Simple neural networks:} Small architectures
\end{enumerate}

Compare deep learning improvements against these baselines.

\subsection{Debugging Strategy}

\textbf{Step 1: Overfit a small dataset}
\begin{itemize}
    \item Take 10-100 examples
    \item Turn off regularization
    \item If can't overfit, model has bugs
\end{itemize}

\textbf{Step 2: Check intermediate outputs}
\begin{itemize}
    \item Visualize activations
    \item Check gradient magnitudes
    \item Verify loss decreases on training set
\end{itemize}

\textbf{Step 3: Diagnose underfitting vs. overfitting}
\begin{itemize}
    \item \textbf{Underfitting:} Poor train performance $\to$ increase capacity
    \item \textbf{Overfitting:} Good train, poor validation $\to$ add regularization
\end{itemize}

\subsection{Common Issues}

\textbf{Vanishing/exploding gradients:}
\begin{itemize}
    \item Use batch normalization
    \item Gradient clipping
    \item Better initialization
\end{itemize}

\textbf{Dead ReLUs:}
\begin{itemize}
    \item Lower learning rate
    \item Use Leaky ReLU or ELU
\end{itemize}

\textbf{Loss not decreasing:}
\begin{itemize}
    \item Check learning rate (too high or too low)
    \item Verify gradient computation
    \item Check data preprocessing
\end{itemize}

\section{Hyperparameter Tuning}
\label{sec:hyperparameter-tuning}

\subsection{Key Hyperparameters (Priority Order)}

\begin{enumerate}
    \item \textbf{Learning rate:} Most critical
    \item \textbf{Network architecture:} Layers, neurons
    \item \textbf{Batch size:} Affects training dynamics
    \item \textbf{Regularization:} Dropout, weight decay
    \item \textbf{Optimizer parameters:} Momentum, beta values
\end{enumerate}

\subsection{Search Strategies}

\textbf{Manual Search:}
\begin{itemize}
    \item Start with educated guesses
    \item Adjust based on results
    \item Time-consuming but insightful
\end{itemize}

\textbf{Grid Search:}
\begin{itemize}
    \item Try all combinations from predefined values
    \item Exhaustive but expensive
    \item Better for 2-3 hyperparameters
\end{itemize}

\textbf{Random Search:}
\begin{itemize}
    \item Sample hyperparameters randomly
    \item More efficient than grid search
    \item Better for high-dimensional spaces
\end{itemize}

\textbf{Bayesian Optimization:}
\begin{itemize}
    \item Model hyperparameter performance
    \item Choose next trials intelligently
    \item More sample-efficient
\end{itemize}

\subsection{Best Practices}

\begin{itemize}
    \item Use logarithmic scale for learning rate
    \item Try learning rates: 0.1, 0.01, 0.001, 0.0001
    \item Start with standard architectures
    \item Use validation set for selection
    \item Retrain with best hyperparameters on full train set
\end{itemize}

\section{Data Preparation and Preprocessing}
\label{sec:data-preparation}

\subsection{Data Splitting}

\textbf{Train/Validation/Test split:}
\begin{itemize}
    \item Training: 60-80\%
    \item Validation: 10-20\%
    \item Test: 10-20\%
\end{itemize}

\textbf{Cross-validation:} For small datasets
\begin{itemize}
    \item k-fold cross-validation
    \item Stratified splits for imbalanced data
\end{itemize}

\subsection{Normalization}

\textbf{Min-Max Scaling:}
\begin{equation}
x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}}
\end{equation}

\textbf{Standardization (Z-score):}
\begin{equation}
x' = \frac{x - \mu}{\sigma}
\end{equation}

Always compute statistics on training set only!

\subsection{Handling Imbalanced Data}

\begin{itemize}
    \item \textbf{Oversampling:} Duplicate minority class examples
    \item \textbf{Undersampling:} Remove majority class examples
    \item \textbf{SMOTE:} Synthetic minority oversampling
    \item \textbf{Class weights:} Penalize errors on minority class more
    \item \textbf{Focal loss:} Focus on hard examples
\end{itemize}

\subsection{Data Augmentation}

Generate additional training examples through transformations (see Chapter 7).

\section{Production Considerations}
\label{sec:production}

\subsection{Model Deployment}

\begin{itemize}
    \item Model compression (pruning, quantization)
    \item Model serving infrastructure
    \item Latency requirements
    \item Batch vs. online inference
\end{itemize}

\subsection{Monitoring}

Track in production:
\begin{itemize}
    \item Prediction distribution shifts
    \item Model performance metrics
    \item System latency and throughput
    \item Error analysis
\end{itemize}

\subsection{Iterative Improvement}

\begin{enumerate}
    \item Deploy initial model
    \item Monitor performance
    \item Collect more data
    \item Retrain and improve
    \item A/B test new models
\end{enumerate}
