% Chapter 14: Autoencoders

\chapter{Autoencoders}
\label{chap:autoencoders}

This chapter explores autoencoders, neural networks designed for unsupervised learning through data reconstruction.

\section{Undercomplete Autoencoders}
\label{sec:undercomplete-ae}

\subsection{Architecture}

An autoencoder consists of:
\begin{itemize}
    \item \textbf{Encoder:} $\vect{h} = f(\vect{x})$ maps input to latent representation
    \item \textbf{Decoder:} $\hat{\vect{x}} = g(\vect{h})$ reconstructs from latent code
\end{itemize}

\subsection{Training Objective}

Minimize reconstruction error:
\begin{equation}
L = \|\vect{x} - g(f(\vect{x}))\|^2
\end{equation}

or more generally:
\begin{equation}
L = -\log p(\vect{x} | g(f(\vect{x})))
\end{equation}

\subsection{Undercomplete Constraint}

If $\dim(\vect{h}) < \dim(\vect{x})$, the autoencoder learns compressed representation.

Acts as dimensionality reduction (similar to PCA but non-linear).

\section{Regularized Autoencoders}
\label{sec:regularized-ae}

\subsection{Sparse Autoencoders}

Add sparsity penalty on hidden activations:
\begin{equation}
L = \|\vect{x} - \hat{\vect{x}}\|^2 + \lambda \sum_j |h_j|
\end{equation}

Encourages learning of sparse, interpretable features.

\subsection{Denoising Autoencoders (DAE)}

Train to reconstruct clean input from corrupted version:
\begin{enumerate}
    \item Corrupt input: $\tilde{\vect{x}} \sim q(\tilde{\vect{x}}|\vect{x})$
    \item Encode corrupted input: $\vect{h} = f(\tilde{\vect{x}})$
    \item Decode and reconstruct: $\hat{\vect{x}} = g(\vect{h})$
    \item Minimize: $L = \|\vect{x} - \hat{\vect{x}}\|^2$
\end{enumerate}

\textbf{Corruption types:}
\begin{itemize}
    \item Additive Gaussian noise
    \item Masking (randomly set inputs to zero)
    \item Salt-and-pepper noise
\end{itemize}

Learns robust representations.

\subsection{Contractive Autoencoders (CAE)}

Add penalty on Jacobian of encoder:
\begin{equation}
L = \|\vect{x} - \hat{\vect{x}}\|^2 + \lambda \left\|\frac{\partial f(\vect{x})}{\partial \vect{x}}\right\|_F^2
\end{equation}

Encourages locally contractive mappings (robust to small perturbations).

\section{Variational Autoencoders}
\label{sec:vae}

\subsection{Probabilistic Framework}

VAE is a generative model:
\begin{align}
p(\vect{x}) &= \int p(\vect{x}|\vect{z}) p(\vect{z}) d\vect{z} \\
p(\vect{z}) &= \mathcal{N}(\boldsymbol{0}, \mat{I}) \\
p(\vect{x}|\vect{z}) &= \mathcal{N}(\vect{x}; \boldsymbol{\mu}_{\theta}(\vect{z}), \boldsymbol{\sigma}^2_{\theta}(\vect{z})\mat{I})
\end{align}

\subsection{Evidence Lower Bound (ELBO)}

Cannot directly maximize $\log p(\vect{x})$. Instead maximize ELBO:
\begin{equation}
\mathcal{L} = \mathbb{E}_{q(\vect{z}|\vect{x})}[\log p(\vect{x}|\vect{z})] - D_{KL}(q(\vect{z}|\vect{x}) \| p(\vect{z}))
\end{equation}

where $q(\vect{z}|\vect{x}) = \mathcal{N}(\vect{z}; \boldsymbol{\mu}_{\phi}(\vect{x}), \boldsymbol{\sigma}^2_{\phi}(\vect{x})\mat{I})$ is the encoder.

\subsection{Reparameterization Trick}

To backpropagate through sampling:
\begin{equation}
\vect{z} = \boldsymbol{\mu}_{\phi}(\vect{x}) + \boldsymbol{\sigma}_{\phi}(\vect{x}) \odot \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \mat{I})
\end{equation}

Enables end-to-end gradient-based training.

\subsection{Generation}

Sample from prior $\vect{z} \sim \mathcal{N}(\boldsymbol{0}, \mat{I})$ and decode to generate new data.

\section{Applications of Autoencoders}
\label{sec:ae-applications}

\subsection{Dimensionality Reduction}

Learn compact representations for:
\begin{itemize}
    \item Visualization (like t-SNE, UMAP)
    \item Preprocessing for downstream tasks
    \item Feature extraction
\end{itemize}

\subsection{Anomaly Detection}

High reconstruction error indicates anomalies:
\begin{itemize}
    \item Fraud detection
    \item Manufacturing defects
    \item Network intrusion detection
\end{itemize}

\subsection{Denoising}

DAEs remove noise from:
\begin{itemize}
    \item Images
    \item Audio signals
    \item Sensor data
\end{itemize}

\subsection{Data Generation}

VAEs generate new samples:
\begin{itemize}
    \item Image synthesis
    \item Data augmentation
    \item Molecular design
\end{itemize}

\subsection{Representation Learning}

Pre-train encoders for:
\begin{itemize}
    \item Transfer learning
    \item Semi-supervised learning
    \item Metric learning
\end{itemize}
