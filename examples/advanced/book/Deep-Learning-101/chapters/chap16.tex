% Chapter 16: Structured Probabilistic Models for Deep Learning

\chapter{Structured Probabilistic Models for Deep Learning}
\label{chap:structured-probabilistic-models}

This chapter covers graphical models and their integration with deep learning.

\section{Graphical Models}
\label{sec:graphical-models}

\subsection{Motivation}

Graphical models represent complex probability distributions using graphs:
\begin{itemize}
    \item Nodes: Random variables
    \item Edges: Probabilistic dependencies
\end{itemize}

\subsection{Bayesian Networks}

\textbf{Directed acyclic graphs} (DAGs) represent conditional dependencies:
\begin{equation}
p(\vect{x}) = \prod_{i=1}^{n} p(x_i | \text{Pa}(x_i))
\end{equation}

where $\text{Pa}(x_i)$ are parents of $x_i$.

\textbf{Example:} Naive Bayes classifier
\begin{equation}
p(y, \vect{x}) = p(y) \prod_{i=1}^{d} p(x_i|y)
\end{equation}

\subsection{Markov Random Fields}

\textbf{Undirected graphs} with potential functions:
\begin{equation}
p(\vect{x}) = \frac{1}{Z} \prod_{c \in \mathcal{C}} \psi_c(\vect{x}_c)
\end{equation}

where $\mathcal{C}$ are cliques and $Z$ is the partition function.

\textbf{Example:} Ising model, Conditional Random Fields (CRFs)

\section{Inference in Graphical Models}
\label{sec:inference}

\subsection{Exact Inference}

\textbf{Variable elimination:} Marginalize variables sequentially

\textbf{Belief propagation:} Message passing on tree-structured graphs

Complexity exponential in tree-width, often intractable.

\subsection{Approximate Inference}

\textbf{Variational inference:} Optimize tractable approximation (Chapter 19)

\textbf{Sampling methods:} Monte Carlo approaches (Chapter 17)

\textbf{Loopy belief propagation:} Approximate inference on graphs with cycles

\section{Deep Learning and Structured Models}
\label{sec:deep-structured}

\subsection{Structured Output Prediction}

Use graphical models to model output structure:

\textbf{Conditional Random Fields (CRFs):}
\begin{equation}
p(\vect{y}|\vect{x}) = \frac{1}{Z(\vect{x})} \exp\left(\sum_c \vect{w}^\top \vect{\phi}_c(\vect{x}, \vect{y}_c)\right)
\end{equation}

\textbf{Applications:}
\begin{itemize}
    \item Sequence labeling (NER, POS tagging)
    \item Image segmentation
    \item Parsing
\end{itemize}

\subsection{Structured Prediction with Neural Networks}

Combine neural networks with graphical models:
\begin{itemize}
    \item \textbf{Feature extraction:} CNN/RNN extracts features
    \item \textbf{Structured inference:} CRF layer for structured output
    \item End-to-end training with backpropagation
\end{itemize}

\textbf{Example:} CNN-CRF for semantic segmentation

\subsection{Neural Module Networks}

Compose neural modules based on program structure for visual reasoning.

\subsection{Graph Neural Networks}

Operate directly on graph-structured data:
\begin{equation}
\vect{h}_v^{(k+1)} = \sigma\left(\mat{W}^{(k)} \sum_{u \in \mathcal{N}(v)} \frac{\vect{h}_u^{(k)}}{|\mathcal{N}(v)|} + \vect{b}^{(k)}\right)
\end{equation}

\textbf{Applications:}
\begin{itemize}
    \item Social networks
    \item Molecular property prediction
    \item Knowledge graphs
    \item Recommendation systems
\end{itemize}
