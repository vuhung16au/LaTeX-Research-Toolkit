% Chapter 13: Linear Factor Models

\chapter{Linear Factor Models}
\label{chap:linear-factor-models}

This chapter introduces probabilistic models with linear structure, which form the foundation for many unsupervised learning methods.

\section{Probabilistic PCA}
\label{sec:prob-pca}

\subsection{Principal Component Analysis Review}

PCA finds orthogonal directions of maximum variance:
\begin{equation}
\vect{z} = \mat{W}^\top (\vect{x} - \boldsymbol{\mu})
\end{equation}

where $\mat{W}$ contains principal components (eigenvectors of covariance matrix).

\subsection{Probabilistic Formulation}

Model observations as:
\begin{align}
\vect{z} &\sim \mathcal{N}(\boldsymbol{0}, \mat{I}) \\
\vect{x} | \vect{z} &\sim \mathcal{N}(\mat{W}\vect{z} + \boldsymbol{\mu}, \sigma^2 \mat{I})
\end{align}

Marginalizing over $\vect{z}$:
\begin{equation}
\vect{x} \sim \mathcal{N}(\boldsymbol{\mu}, \mat{W}\mat{W}^\top + \sigma^2 \mat{I})
\end{equation}

\subsection{Learning}

Maximize likelihood using EM algorithm:
\begin{itemize}
    \item \textbf{E-step:} Compute $p(\vect{z}|\vect{x})$
    \item \textbf{M-step:} Update $\mat{W}$, $\boldsymbol{\mu}$, $\sigma^2$
\end{itemize}

As $\sigma^2 \to 0$, recovers standard PCA.

\section{Factor Analysis}
\label{sec:factor-analysis}

Similar to probabilistic PCA but with diagonal noise covariance:
\begin{equation}
\vect{x} | \vect{z} \sim \mathcal{N}(\mat{W}\vect{z} + \boldsymbol{\mu}, \boldsymbol{\Psi})
\end{equation}

where $\boldsymbol{\Psi}$ is diagonal. Each observed dimension has its own noise variance.

\textbf{Applications:} Psychology, social sciences, finance

\section{Independent Component Analysis}
\label{sec:ica}

\subsection{Objective}

Find independent sources from linear mixtures:
\begin{equation}
\vect{x} = \mat{A}\vect{s}
\end{equation}

where $\vect{s}$ contains independent sources.

\subsection{Non-Gaussianity}

ICA exploits that independent signals are typically non-Gaussian.

\textbf{Applications:}
\begin{itemize}
    \item Blind source separation (cocktail party problem)
    \item Signal processing
    \item Feature extraction
\end{itemize}

\section{Sparse Coding}
\label{sec:sparse-coding}

Learn overcomplete dictionary where data has sparse representation:
\begin{equation}
\min_{\mat{D}, \vect{z}} \|\vect{x} - \mat{D}\vect{z}\|^2 + \lambda \|\vect{z}\|_1
\end{equation}

\textbf{Applications:}
\begin{itemize}
    \item Image denoising
    \item Feature learning
    \item Compression
\end{itemize}
