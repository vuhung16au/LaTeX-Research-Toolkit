% Chapter 6: Deep Feedforward Networks

\chapter{Deep Feedforward Networks}
\label{chap:feedforward-networks}

This chapter introduces deep feedforward neural networks, also known as multilayer perceptrons (MLPs). These are the fundamental building blocks of deep learning.

\section{Introduction to Feedforward Networks}
\label{sec:intro-feedforward}

A \textbf{feedforward neural network} approximates a function $f^*$. For input $\vect{x}$, the network computes $y = f(\vect{x}; \vect{\theta})$ and learns parameters $\vect{\theta}$ such that $f \approx f^*$.

\subsection{Network Architecture}

A feedforward network consists of layers:
\begin{itemize}
    \item \textbf{Input layer:} receives raw features $\vect{x}$
    \item \textbf{Hidden layers:} intermediate representations $\vect{h}^{(1)}, \vect{h}^{(2)}, \ldots$
    \item \textbf{Output layer:} produces predictions $\hat{y}$
\end{itemize}

For a network with $L$ layers:

\begin{equation}
\vect{h}^{(l)} = \sigma(\mat{W}^{(l)} \vect{h}^{(l-1)} + \vect{b}^{(l)})
\end{equation}

where $\vect{h}^{(0)} = \vect{x}$, $\mat{W}^{(l)}$ are weights, $\vect{b}^{(l)}$ are biases, and $\sigma$ is an activation function.

\subsection{Forward Propagation}

The computation proceeds from input to output:

\begin{align}
\vect{z}^{(1)} &= \mat{W}^{(1)} \vect{x} + \vect{b}^{(1)} \\
\vect{h}^{(1)} &= \sigma(\vect{z}^{(1)}) \\
\vect{z}^{(2)} &= \mat{W}^{(2)} \vect{h}^{(1)} + \vect{b}^{(2)} \\
&\vdots \\
\hat{y} &= \vect{h}^{(L)}
\end{align}

\subsection{Universal Approximation}

The \textbf{universal approximation theorem} states that a feedforward network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of $\mathbb{R}^n$, given appropriate activation functions.

However, deeper networks often learn more efficiently.

\section{Activation Functions}
\label{sec:activation-functions}

Activation functions introduce non-linearity, enabling networks to learn complex patterns.

\subsection{Sigmoid}

\begin{equation}
\sigma(z) = \frac{1}{1 + e^{-z}}
\end{equation}

Properties:
\begin{itemize}
    \item Range: $(0, 1)$
    \item Saturates for large $|z|$ (vanishing gradients)
    \item Not zero-centered
    \item Historically important but less common in hidden layers
\end{itemize}

\subsection{Hyperbolic Tangent (tanh)}

\begin{equation}
\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
\end{equation}

Properties:
\begin{itemize}
    \item Range: $(-1, 1)$
    \item Zero-centered
    \item Still suffers from saturation
\end{itemize}

\subsection{Rectified Linear Unit (ReLU)}

\begin{equation}
\text{ReLU}(z) = \max(0, z)
\end{equation}

Properties:
\begin{itemize}
    \item Simple and computationally efficient
    \item No saturation for positive values
    \item Can cause "dead neurons" (always output 0)
    \item Most widely used activation
\end{itemize}

\subsection{Leaky ReLU and Variants}

\textbf{Leaky ReLU:}
\begin{equation}
\text{LeakyReLU}(z) = \max(\alpha z, z), \quad \alpha \ll 1
\end{equation}

\textbf{Parametric ReLU (PReLU):}
\begin{equation}
\text{PReLU}(z) = \max(\alpha z, z)
\end{equation}
where $\alpha$ is learned.

\textbf{Exponential Linear Unit (ELU):}
\begin{equation}
\text{ELU}(z) = \begin{cases}
z & \text{if } z > 0 \\
\alpha(e^z - 1) & \text{if } z \leq 0
\end{cases}
\end{equation}

\subsection{Swish and GELU}

\textbf{Swish:}
\begin{equation}
\text{Swish}(z) = z \cdot \sigma(z)
\end{equation}

\textbf{Gaussian Error Linear Unit (GELU):}
\begin{equation}
\text{GELU}(z) = z \cdot \Phi(z)
\end{equation}
where $\Phi$ is the Gaussian CDF. Used in modern transformers.

\section{Output Units and Loss Functions}
\label{sec:output-loss}

The choice of output layer and loss function depends on the task.

\subsection{Linear Output for Regression}

For regression, use linear output:
\begin{equation}
\hat{y} = \mat{W}^\top \vect{h} + b
\end{equation}

with MSE loss:
\begin{equation}
L = \frac{1}{n} \sum_{i=1}^{n} (y^{(i)} - \hat{y}^{(i)})^2
\end{equation}

\subsection{Sigmoid Output for Binary Classification}

For binary classification:
\begin{equation}
\hat{y} = \sigma(\mat{W}^\top \vect{h} + b)
\end{equation}

with binary cross-entropy loss:
\begin{equation}
L = -\frac{1}{n} \sum_{i=1}^{n} [y^{(i)} \log \hat{y}^{(i)} + (1-y^{(i)}) \log(1-\hat{y}^{(i)})]
\end{equation}

\subsection{Softmax Output for Multiclass Classification}

For $K$ classes:
\begin{equation}
\hat{y}_k = \frac{\exp(z_k)}{\sum_{j=1}^{K} \exp(z_j)}
\end{equation}

with categorical cross-entropy loss:
\begin{equation}
L = -\frac{1}{n} \sum_{i=1}^{n} \sum_{k=1}^{K} y_k^{(i)} \log \hat{y}_k^{(i)}
\end{equation}

\section{Backpropagation}
\label{sec:backpropagation}

\textbf{Backpropagation} efficiently computes gradients using the chain rule.

\subsection{The Chain Rule}

For composition $f(g(x))$:
\begin{equation}
\frac{df}{dx} = \frac{df}{dg} \frac{dg}{dx}
\end{equation}

For vectors, we use the Jacobian:
\begin{equation}
\frac{\partial \vect{y}}{\partial \vect{x}} = \frac{\partial \vect{y}}{\partial \vect{z}} \frac{\partial \vect{z}}{\partial \vect{x}}
\end{equation}

\subsection{Backward Pass}

Starting from the loss $L$, we compute gradients layer by layer:

\begin{align}
\delta^{(L)} &= \nabla_{\vect{h}^{(L)}} L \\
\delta^{(l)} &= (\mat{W}^{(l+1)})^\top \delta^{(l+1)} \odot \sigma'(\vect{z}^{(l)})
\end{align}

where $\odot$ denotes element-wise multiplication.

Parameter gradients:
\begin{align}
\frac{\partial L}{\partial \mat{W}^{(l)}} &= \delta^{(l)} (\vect{h}^{(l-1)})^\top \\
\frac{\partial L}{\partial \vect{b}^{(l)}} &= \delta^{(l)}
\end{align}

\subsection{Computational Graph}

Modern frameworks use automatic differentiation on computational graphs:
\begin{itemize}
    \item \textbf{Forward mode:} efficient when outputs $\gg$ inputs
    \item \textbf{Reverse mode (backprop):} efficient when inputs $\gg$ outputs
\end{itemize}

\section{Design Choices}
\label{sec:design-choices}

\subsection{Network Depth and Width}

\textbf{Depth} (number of layers):
\begin{itemize}
    \item Deeper networks can learn more complex functions
    \item Can be harder to optimize (vanishing/exploding gradients)
    \item Modern techniques enable very deep networks (100+ layers)
\end{itemize}

\textbf{Width} (neurons per layer):
\begin{itemize}
    \item Wider networks have more capacity
    \item Trade-off between width and depth
    \item Very wide shallow networks vs. narrow deep networks
\end{itemize}

\subsection{Weight Initialization}

Poor initialization can prevent learning. Common strategies:

\textbf{Xavier/Glorot initialization:}
\begin{equation}
W_{ij} \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right)
\end{equation}

\textbf{He initialization} (for ReLU):
\begin{equation}
W_{ij} \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}}}\right)
\end{equation}

\subsection{Batch Training}

\textbf{Mini-batch gradient descent:}
\begin{itemize}
    \item Compute gradients on small batches (typically 32-256 examples)
    \item Provides regularization through noise
    \item Enables efficient parallel computation
    \item Balances between SGD and full-batch GD
\end{itemize}
