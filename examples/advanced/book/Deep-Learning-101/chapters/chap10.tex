% Chapter 10: Sequence Modeling: Recurrent and Recursive Nets

\chapter{Sequence Modeling: Recurrent and Recursive Nets}
\label{chap:sequence-modeling}

This chapter covers architectures designed for sequential and temporal data, including recurrent neural networks (RNNs) and their variants.

\section{Recurrent Neural Networks}
\label{sec:rnns}

\subsection{Motivation}

Sequential data has temporal dependencies:
\begin{itemize}
    \item Time series (stock prices, sensor readings)
    \item Text (words depend on previous words)
    \item Speech (phonemes form words)
    \item Video (frames over time)
\end{itemize}

Standard feedforward networks cannot capture these dependencies effectively.

\subsection{Basic RNN Architecture}

An RNN maintains a hidden state $\vect{h}_t$ that evolves over time:

\begin{align}
\vect{h}_t &= \sigma(\mat{W}_{hh} \vect{h}_{t-1} + \mat{W}_{xh} \vect{x}_t + \vect{b}_h) \\
\vect{y}_t &= \mat{W}_{hy} \vect{h}_t + \vect{b}_y
\end{align}

where $\vect{x}_t$ is input at time $t$, and $\sigma$ is typically tanh.

\subsection{Unfolding in Time}

RNNs can be "unrolled" into a feedforward network with shared weights across time steps:

\begin{equation}
\vect{h}_t = f(\vect{h}_{t-1}, \vect{x}_t; \vect{\theta})
\end{equation}

\subsection{Types of Sequences}

\textbf{One-to-many:} Single input, sequence output (e.g., image captioning)

\textbf{Many-to-one:} Sequence input, single output (e.g., sentiment classification)

\textbf{Many-to-many:} Sequence input and output (e.g., machine translation)

\section{Backpropagation Through Time}
\label{sec:bptt}

\subsection{BPTT Algorithm}

Gradients are computed by unrolling the network and applying backpropagation:

\begin{equation}
\frac{\partial L}{\partial \vect{h}_t} = \frac{\partial L}{\partial \vect{y}_t} \frac{\partial \vect{y}_t}{\partial \vect{h}_t} + \frac{\partial L}{\partial \vect{h}_{t+1}} \frac{\partial \vect{h}_{t+1}}{\partial \vect{h}_t}
\end{equation}

For weight matrix $\mat{W}$:
\begin{equation}
\frac{\partial L}{\partial \mat{W}} = \sum_{t=1}^{T} \frac{\partial L_t}{\partial \mat{W}}
\end{equation}

\subsection{Vanishing and Exploding Gradients}

Gradients can vanish or explode exponentially:

\begin{equation}
\frac{\partial \vect{h}_t}{\partial \vect{h}_k} = \prod_{i=k+1}^{t} \frac{\partial \vect{h}_i}{\partial \vect{h}_{i-1}} = \prod_{i=k+1}^{t} \mat{W}^\top \text{diag}(\sigma'(\vect{z}_i))
\end{equation}

If eigenvalues of $\mat{W}$ are:
\begin{itemize}
    \item $< 1$: gradients vanish
    \item $> 1$: gradients explode
\end{itemize}

\textbf{Solutions:}
\begin{itemize}
    \item Gradient clipping (for explosion)
    \item Careful initialization
    \item ReLU activation
    \item LSTM/GRU architectures
\end{itemize}

\subsection{Truncated BPTT}

For very long sequences, truncate gradient computation:
\begin{itemize}
    \item Only backpropagate through $k$ time steps
    \item Reduces memory and computation
    \item Trade-off: cannot capture long-term dependencies
\end{itemize}

\section{Long Short-Term Memory (LSTM)}
\label{sec:lstm}

\subsection{Architecture}

LSTM uses \textbf{gating mechanisms} to control information flow:

\begin{align}
\vect{f}_t &= \sigma(\mat{W}_f [\vect{h}_{t-1}, \vect{x}_t] + \vect{b}_f) \quad \text{(forget gate)} \\
\vect{i}_t &= \sigma(\mat{W}_i [\vect{h}_{t-1}, \vect{x}_t] + \vect{b}_i) \quad \text{(input gate)} \\
\tilde{\vect{c}}_t &= \tanh(\mat{W}_c [\vect{h}_{t-1}, \vect{x}_t] + \vect{b}_c) \quad \text{(candidate)} \\
\vect{c}_t &= \vect{f}_t \odot \vect{c}_{t-1} + \vect{i}_t \odot \tilde{\vect{c}}_t \quad \text{(cell state)} \\
\vect{o}_t &= \sigma(\mat{W}_o [\vect{h}_{t-1}, \vect{x}_t] + \vect{b}_o) \quad \text{(output gate)} \\
\vect{h}_t &= \vect{o}_t \odot \tanh(\vect{c}_t) \quad \text{(hidden state)}
\end{align}

\subsection{Key Ideas}

\textbf{Cell state} $\vect{c}_t$: Long-term memory
\begin{itemize}
    \item Information flows with minimal transformation
    \item Gates control what to remember/forget
\end{itemize}

\textbf{Forget gate} $\vect{f}_t$: Decides what to discard from cell state

\textbf{Input gate} $\vect{i}_t$: Decides what new information to store

\textbf{Output gate} $\vect{o}_t$: Decides what to output

\subsection{Advantages}

\begin{itemize}
    \item Addresses vanishing gradient problem
    \item Can learn long-term dependencies
    \item Gradients flow more easily through cell state
    \item Widely used for sequential tasks
\end{itemize}

\section{Gated Recurrent Units (GRU)}
\label{sec:gru}

\subsection{Architecture}

GRU simplifies LSTM with fewer parameters:

\begin{align}
\vect{z}_t &= \sigma(\mat{W}_z [\vect{h}_{t-1}, \vect{x}_t]) \quad \text{(update gate)} \\
\vect{r}_t &= \sigma(\mat{W}_r [\vect{h}_{t-1}, \vect{x}_t]) \quad \text{(reset gate)} \\
\tilde{\vect{h}}_t &= \tanh(\mat{W} [\vect{r}_t \odot \vect{h}_{t-1}, \vect{x}_t]) \quad \text{(candidate)} \\
\vect{h}_t &= (1 - \vect{z}_t) \odot \vect{h}_{t-1} + \vect{z}_t \odot \tilde{\vect{h}}_t
\end{align}

\subsection{Comparison with LSTM}

\textbf{GRU:}
\begin{itemize}
    \item Fewer parameters (faster to train)
    \item No separate cell state
    \item Often performs similarly to LSTM
\end{itemize}

\textbf{LSTM:}
\begin{itemize}
    \item More expressive (separate forget/input gates)
    \item Better for longer sequences in some cases
\end{itemize}

\section{Sequence-to-Sequence Models}
\label{sec:seq2seq}

\subsection{Encoder-Decoder Architecture}

For tasks like machine translation:

\textbf{Encoder:} Processes input sequence into fixed representation
\begin{equation}
\vect{c} = f(\vect{x}_1, \vect{x}_2, \ldots, \vect{x}_T)
\end{equation}

\textbf{Decoder:} Generates output sequence from representation
\begin{equation}
\vect{y}_t = g(\vect{y}_{t-1}, \vect{c}, \vect{s}_{t-1})
\end{equation}

\subsection{Attention Mechanism}

Standard seq2seq compresses entire input into fixed vector $\vect{c}$, causing information bottleneck.

\textbf{Attention} allows decoder to focus on relevant input parts:

\begin{align}
e_{ti} &= a(\vect{s}_{t-1}, \vect{h}_i) \quad \text{(alignment scores)} \\
\alpha_{ti} &= \frac{\exp(e_{ti})}{\sum_j \exp(e_{tj})} \quad \text{(attention weights)} \\
\vect{c}_t &= \sum_i \alpha_{ti} \vect{h}_i \quad \text{(context vector)}
\end{align}

Benefits:
\begin{itemize}
    \item Dynamic context for each output
    \item Better for long sequences
    \item Interpretable (visualize attention weights)
\end{itemize}

\subsection{Applications}

\begin{itemize}
    \item Machine translation
    \item Text summarization
    \item Question answering
    \item Image captioning (CNN encoder, RNN decoder)
    \item Speech recognition
\end{itemize}

\section{Advanced Topics}
\label{sec:rnn-advanced}

\subsection{Bidirectional RNNs}

Process sequence in both directions:
\begin{align}
\overrightarrow{\vect{h}}_t &= f(\vect{x}_t, \overrightarrow{\vect{h}}_{t-1}) \\
\overleftarrow{\vect{h}}_t &= f(\vect{x}_t, \overleftarrow{\vect{h}}_{t+1}) \\
\vect{h}_t &= [\overrightarrow{\vect{h}}_t; \overleftarrow{\vect{h}}_t]
\end{align}

Useful when future context is available.

\subsection{Deep RNNs}

Stack multiple RNN layers:
\begin{equation}
\vect{h}_t^{(l)} = f(\vect{h}_t^{(l-1)}, \vect{h}_{t-1}^{(l)})
\end{equation}

Each layer captures different levels of abstraction.

\subsection{Teacher Forcing}

During training, use ground truth as decoder input (not model's prediction):
\begin{itemize}
    \item Faster convergence
    \item Stable training
    \item May cause exposure bias at test time
\end{itemize}

\subsection{Beam Search}

For inference, maintain top-$k$ hypotheses:
\begin{itemize}
    \item Better than greedy decoding
    \item Trade-off between quality and speed
    \item Common beam size: 5-10
\end{itemize}
